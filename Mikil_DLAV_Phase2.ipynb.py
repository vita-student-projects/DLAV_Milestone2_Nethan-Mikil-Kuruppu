# -*- coding: utf-8 -*-
"""DLAV_Phase2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jpr07VUO_ZjF2YOuxr9lKO3U9jvZ6q2-

# Phase 2: Trajectory Prediction with Auxiliary Depth Estimation

# üß≠ Introduction

'''
Welcome to **Phase 2** of the DLAV Projec! üöóüí®

In this phase, you'll work with a more challenging dataset that includes:
- RGB **camera images**
- Ground-truth **depth maps**
- Ground-truth **semantic segmentation** labels

Your goal is still to predict the **future trajectory** of the self-driving car (SDC), but you now have more tools at your disposal! üéØ

Here, we provide an example where **depth estimation** is used as an auxiliary task to improve trajectory prediction.

However, you're **free to explore** other auxiliary tasks (e.g., using semantic labels), different loss functions, data augmentations, or better architectures! üí°

This notebook will walk you through loading the dataset, building a model, training with and without the auxiliary task, and visualizing results.
'''
"""

# Install gdown to handle Google Drive file download
#!pip install -q gdown

import gdown
import zipfile

download_url = f"https://drive.google.com/uc?id=1YkGwaxBKNiYL2nq--cB6WMmYGzRmRKVr"
output_zip = "dlav_train.zip"
gdown.download(download_url, output_zip, quiet=False)  # Downloads the file to your drive
with zipfile.ZipFile(output_zip, 'r') as zip_ref:  # Extracts the downloaded zip file
    zip_ref.extractall(".")

download_url = "https://drive.google.com/uc?id=1wtmT_vH9mMUNOwrNOMFP6WFw6e8rbOdu"
output_zip = "dlav_val.zip"
gdown.download(download_url, output_zip, quiet=False)
with zipfile.ZipFile(output_zip, 'r') as zip_ref:
    zip_ref.extractall(".")

download_url = "https://drive.google.com/uc?id=1G9xGE7s-Ikvvc2-LZTUyuzhWAlNdLTLV"
output_zip = "dlav_test_public.zip"
gdown.download(download_url, output_zip, quiet=False)
with zipfile.ZipFile(output_zip, 'r') as zip_ref:
    zip_ref.extractall(".")

"""## üìÇ The Dataset

We are now working with a richer dataset that includes not just images and trajectories,
but also **depth maps** (and semantic segmentation labels, though unused in this example).

The data is stored in `.pkl` files and each file contains:
- `camera`: RGB image (shape: H x W x 3)
- `sdc_history_feature`: the past trajectory of the car
- `sdc_future_feature`: the future trajectory to predict
- `depth`: ground truth depth map (shape: H x W x 1)

We'll define a `DrivingDataset` class to load and return these tensors in a format our model can work with.
"""

import os
import torch
import pickle
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import csv
import random
import timm
from timm.models.vision_transformer import Block
import math 
import torchvision.transforms as transforms
seed = 42
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

class DrivingAugmentation:
    def __init__(self, p_aug=0.8):
        self.p_aug = p_aug
        
        self.color_transform = transforms.ColorJitter(
            brightness=0.2,
            contrast=0.2,
            saturation=0.2,
            hue=0.05
        )
        
    def __call__(self, sample):
        if torch.rand(1) > self.p_aug:
            return sample
        
        if torch.rand(1) > 0.5:
            sample = self._apply_rotation(sample)
            
        if torch.rand(1) > 0.5:
            sample = self._apply_color_jitter(sample)
            
        if torch.rand(1) > 0.5:
            sample = self._apply_trajectory_noise(sample)
            
        return sample
    
    def _apply_rotation(self, sample, max_angle=5.0):
        angle = (torch.rand(1) - 0.5) * 2 * max_angle * (3.14159 / 180)
        cos_a, sin_a = torch.cos(angle), torch.sin(angle)
        
        R = torch.tensor([[cos_a, -sin_a], 
                          [sin_a, cos_a]], dtype=torch.float32)
        
        sample['history'][:, :2] = (R @ sample['history'][:, :2].T).T
        
        if 'future' in sample:
            sample['future'][:, :2] = (R @ sample['future'][:, :2].T).T
            
        if abs(angle) > 0.05:
            sample['camera'] = torch.flip(sample['camera'], dims=[2])
            
        return sample
    
    def _apply_color_jitter(self, sample):
        sample['camera'] = self.color_transform(sample['camera'])
        return sample
    
    def _apply_trajectory_noise(self, sample):
        history_noise = torch.randn_like(sample['history'][:, :2]) * 0.02
        sample['history'][:, :2] = sample['history'][:, :2] + history_noise
        return sample

class EnhancedDrivingDataset(Dataset):
    def __init__(self, file_list, test=False, augment=True):
        self.samples = file_list
        self.test = test
        self.augment = DrivingAugmentation(p_aug=0.8) if augment and not test else None

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        with open(self.samples[idx], 'rb') as f:
            data = pickle.load(f)

        camera = torch.FloatTensor(data['camera']).permute(2, 0, 1) / 255.0
        history = torch.FloatTensor(data['sdc_history_feature'])
        depth = torch.FloatTensor(data['depth'])
        
        sample = {
            'camera': camera,
            'history': history,
            'depth': depth
        }
        
        if not self.test:
            future = torch.FloatTensor(data['sdc_future_feature'])
            semantic = torch.LongTensor(data['semantic_label'])
            sample['future'] = future
            sample['semantic'] = semantic
        
        if self.augment is not None:
            sample = self.augment(sample)
            
        return sample


"""## üß† The Model: Trajectory + Depth Prediction

We've extended our trajectory prediction model to optionally include a **depth estimation decoder**.

Why?
- Predicting depth helps the model **learn richer visual features** from the camera input.
- This acts as a form of **multi-task learning**, where learning to estimate depth reinforces scene understanding, ultimately leading to better trajectory predictions.
- This can be especially useful in complex environments with occlusions or sharp turns.

The model has:
- A CNN backbone to extract features from the image
- An MLP to process historical trajectory features
- A trajectory decoder to predict future coordinates
- (Optionally) A depth decoder to predict dense depth maps

This auxiliary task is enabled by setting `use_depth_aux=True`.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))
        
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class HistoryEncoder(nn.Module):
    def __init__(self, input_dim=3, hidden_dim=768, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)
        self.projection = nn.Linear(hidden_dim * 2, hidden_dim)
        self.pos_encoding = PositionalEncoding(hidden_dim)
        
    def forward(self, history):
        lstm_out, (h_n, c_n) = self.lstm(history)
        final_hidden = torch.cat([h_n[-2], h_n[-1]], dim=1)
        projected = self.projection(final_hidden)
        
        lstm_out = self.projection(lstm_out)
        lstm_out = self.pos_encoding(lstm_out)
        
        return projected, lstm_out

class ViTDrivingPlanner(nn.Module):
    def __init__(self, use_depth_aux=False, use_semantic_aux=False, 
                 model_name='vit_base_patch16_224'):
        super().__init__()
        self.use_depth_aux = use_depth_aux
        self.use_semantic_aux = use_semantic_aux
        
        self.vit = timm.create_model(model_name, pretrained=True, num_classes=0, img_size=224)
        self.vit_dim = self.vit.embed_dim
        
        self.resize = nn.AdaptiveAvgPool2d((224, 224))
        
        self.history_encoder = HistoryEncoder(input_dim=3, hidden_dim=self.vit_dim)
        
        self.fusion_layers = nn.ModuleList([
            Block(dim=self.vit_dim, num_heads=8, mlp_ratio=4.0, qkv_bias=True)
            for _ in range(3)
        ])
        
        self.trajectory_head = nn.Sequential(
            nn.LayerNorm(self.vit_dim),
            nn.Linear(self.vit_dim, self.vit_dim),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(self.vit_dim, 60 * 3),
        )
        
        if self.use_depth_aux:
            self.depth_decoder = self._build_decoder(self.vit_dim, 1, 'depth')
            
        if self.use_semantic_aux:
            self.semantic_decoder = self._build_decoder(self.vit_dim, 15, 'semantic')
    
    def _build_decoder(self, in_channels, out_channels, task_type):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, 256, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(64, out_channels, 3, 1, 1),
            nn.Upsample(size=(200, 300), mode='bilinear', align_corners=False)
        )
    
    def forward(self, camera, history):
        batch_size = camera.size(0)
        
        camera_resized = self.resize(camera)
        visual_tokens = self.vit.forward_features(camera_resized)
        visual_cls = visual_tokens[:, 0]
        visual_patches = visual_tokens[:, 1:]
        
        history_global, history_sequence = self.history_encoder(history)
        
        combined_tokens = torch.cat([visual_tokens, history_sequence], dim=1)
        for fusion_layer in self.fusion_layers:
            combined_tokens = fusion_layer(combined_tokens)
        
        fused_cls = combined_tokens[:, 0]
        
        trajectory_output = self.trajectory_head(fused_cls)
        trajectory = trajectory_output.view(batch_size, 60, 3)
        
        depth_output = None
        semantic_output = None
        
        if self.use_depth_aux:
            patch_h = patch_w = int(math.sqrt(visual_patches.size(1)))
            visual_spatial = visual_patches.transpose(1, 2).view(
                batch_size, self.vit_dim, patch_h, patch_w
            )
            depth_output = self.depth_decoder(visual_spatial)
            depth_output = depth_output.permute(0, 2, 3, 1)
            
        if self.use_semantic_aux:
            patch_h = patch_w = int(math.sqrt(visual_patches.size(1)))
            visual_spatial = visual_patches.transpose(1, 2).view(
                batch_size, self.vit_dim, patch_h, patch_w
            )
            semantic_output = self.semantic_decoder(visual_spatial)
            semantic_output = semantic_output.permute(0, 2, 3, 1)
        
        return trajectory, depth_output, semantic_output

"""## üèãÔ∏è Training with Auxiliary Loss

The training loop is similar to Phase 1 ‚Äî except now, if enabled, we also compute a loss on the predicted **depth map**.

We define:
- `trajectory_loss` as standard MSE between predicted and true future trajectory
- `depth_loss` as L1 loss between predicted and ground truth depth

Total loss = `trajectory_loss + lambda * depth_loss`

This helps guide the model to learn better representations from visual input. The weight `lambda` is a hyperparameter you can tune!
"""

import torch
import torch.nn.functional as F

class EarlyStopping:
    def __init__(self, patience=15, min_delta=0.005, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.best_ade = float('inf')
        self.counter = 0
        self.early_stop = False
        
    def __call__(self, ade):
        if ade < self.best_ade - self.min_delta:
            self.best_ade = ade
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

def trajectory_specific_loss(pred, target):
    pred_pos = pred[:, :, :2]
    target_pos = target[:, :, :2]
    
    pos_loss = F.smooth_l1_loss(pred_pos, target_pos, beta=0.1)
    
    endpoint_loss = F.mse_loss(pred[:, -1, :2], target[:, -1, :2])
    
    T = pred.size(1)
    weights = torch.exp(-0.05 * torch.arange(T, device=pred.device))
    weights = weights.view(1, -1, 1)
    
    weighted_loss = F.mse_loss(pred_pos * weights, target_pos * weights)
    
    diff_pred = torch.diff(pred_pos, dim=1)
    diff_target = torch.diff(target_pos, dim=1)
    smoothness_loss = F.mse_loss(diff_pred, diff_target)
    
    total_loss = (0.4 * pos_loss +
                  0.3 * endpoint_loss +
                  0.2 * weighted_loss +
                  0.1 * smoothness_loss)
    
    return total_loss

def train_one_epoch_enhanced(model, train_loader, optimizer, device, 
                           lambda_depth=0.1, lambda_semantic=0.5, 
                           use_depth_aux=False, use_semantic_aux=False):
    model.train()
    train_loss = 0.0
    
    for batch in train_loader:
        cam = batch['camera'].to(device)
        hist = batch['history'].to(device)
        fut = batch['future'].to(device)
        
        optimizer.zero_grad()
        fut_pred, dep_pred, sem_pred = model(cam, hist)
        
        traj_loss = trajectory_specific_loss(fut_pred, fut)
        loss = traj_loss
        
        if use_depth_aux and dep_pred is not None:
            dep = batch['depth'].to(device)
            depth_loss = F.l1_loss(dep_pred, dep)
            loss += lambda_depth * depth_loss
            
        if use_semantic_aux and sem_pred is not None:
            sem = batch['semantic'].to(device).long()
            sem_pred_reshaped = sem_pred.permute(0, 3, 1, 2)
            semantic_loss = F.cross_entropy(sem_pred_reshaped, sem)
            loss += lambda_semantic * semantic_loss
        
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        train_loss += loss.item()
    
    return train_loss / len(train_loader)


def validate_enhanced(model, val_loader, device):
    model.eval()
    total_ade, total_fde, total_mse = 0.0, 0.0, 0.0
    count = 0

    with torch.no_grad():
        for batch in val_loader:
            cam = batch['camera'].to(device)
            hist = batch['history'].to(device)
            fut = batch['future'].to(device)

            fut_pred, _, _ = model(cam, hist)

            B, T, _ = fut.shape
            count += B

            ade = torch.norm(fut_pred[:, :, :2] - fut[:, :, :2], dim=2).mean(dim=1).sum()
            fde = torch.norm(fut_pred[:, -1, :2] - fut[:, -1, :2], dim=1).sum()
            mse = F.mse_loss(fut_pred, fut, reduction='sum')

            total_ade += ade.item()
            total_fde += fde.item()
            total_mse += mse.item()

    ade_avg = total_ade / count
    fde_avg = total_fde / count
    mse_avg = total_mse / (count * T * 3)

    return ade_avg, fde_avg, mse_avg

def train(model, train_loader, val_loader, optimizer, num_epochs=50, lambda_depth=0.1, use_depth_aux=False):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    model = model.to(device)

    for epoch in range(num_epochs):
        train_loss = train_one_epoch(model, train_loader, optimizer, device, lambda_depth, use_depth_aux)
        ade, fde, mse = validate(model, val_loader, device)

        print(f"Epoch {epoch+1}, Loss: {train_loss:.4f}, Validation - ADE: {ade:.4f}, FDE: {fde:.4f}, Traj MSE: {mse:.6f}")

import torch.optim as optim
from torch.utils.data import DataLoader
import os

train_data_dir = "train"
val_data_dir = "val"

train_files = [os.path.join(train_data_dir, f) for f in os.listdir(train_data_dir) if f.endswith('.pkl')]
val_files = [os.path.join(val_data_dir, f) for f in os.listdir(val_data_dir) if f.endswith('.pkl')]

train_dataset = EnhancedDrivingDataset(train_files, test=False, augment=True)
val_dataset = EnhancedDrivingDataset(val_files, test=False, augment=False) 

train_loader = DataLoader(train_dataset, batch_size=32, num_workers=2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""## üîç Let's Compare Two Settings

We'll now train and evaluate the model in **two modes**:

1. **Without auxiliary task** ‚Äî the model only predicts the trajectory.
2. **With depth auxiliary task** ‚Äî the model also predicts a depth map, which helps it learn better visual features.

By comparing the results (ADE, FDE, and Trajectory MSE), you'll see the benefit of multi-task learning in action! üöÄ
"""
'''
print("\n" + "="*60)
print("TRAINING ViT MODEL WITHOUT AUXILIARY TASKS")
print("="*60)

model_no_aux = ViTDrivingPlanner(
    use_depth_aux=False, 
    use_semantic_aux=False,
    model_name='vit_base_patch16_224'
)

optimizer_no_aux = optim.AdamW(
    model_no_aux.parameters(), 
    lr=5e-5,
    weight_decay=1e-4
)

scheduler_no_aux = optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer_no_aux, T_0=10, T_mult=2, eta_min=1e-6
)

model_no_aux = model_no_aux.to(device)

best_ade_no_aux = float('inf')
for epoch in range(50):
    train_loss = train_one_epoch_enhanced(
        model_no_aux, train_loader, optimizer_no_aux, device,
        lambda_depth=0.0, lambda_semantic=0.0,
        use_depth_aux=False, use_semantic_aux=False
    )
    
    ade, fde, mse = validate_enhanced(model_no_aux, val_loader, device)
    
    scheduler_no_aux.step()
    
    print(f"Epoch {epoch+1}/50 - Train Loss: {train_loss:.4f}, ADE: {ade:.4f}, FDE: {fde:.4f}, Traj MSE: {mse:.6f}")
    
    if ade < best_ade_no_aux:
        best_ade_no_aux = ade
        torch.save(model_no_aux.state_dict(), "vit_no_aux_model.pth")
'''
print("\n" + "="*60)
print("TRAINING ViT MODEL WITH DEPTH + SEMANTIC AUXILIARY TASKS")
print("="*60)

model_with_aux = ViTDrivingPlanner(
    use_depth_aux=True, 
    use_semantic_aux=True,
    model_name='vit_base_patch16_224'
)

optimizer_with_aux = optim.AdamW(
    model_with_aux.parameters(), 
    lr=5e-5,
    weight_decay=1e-3
)

scheduler_with_aux = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer_with_aux,
    mode='min',
    factor=0.5,
    patience=10,
    threshold=0.005,
    threshold_mode='rel',
    cooldown=3,
    min_lr=1e-8,
    verbose=True
)

model_with_aux = model_with_aux.to(device)

best_ade_with_aux = float('inf')
early_stopping = EarlyStopping(patience=15, min_delta=0.005, verbose=True)
for epoch in range(150):
    train_loss = train_one_epoch_enhanced(
        model_with_aux, train_loader, optimizer_with_aux, device,
        lambda_depth=0.1, lambda_semantic=0.5,
        use_depth_aux=True, use_semantic_aux=True
    )
    
    ade, fde, mse = validate_enhanced(model_with_aux, val_loader, device)

    early_stopping(ade)
    if early_stopping.early_stop:
        print(f"\nEarly stopping at epoch {epoch+1}")
        print(f"Best validation ADE: {early_stopping.best_ade:.4f}")
        break
    
    scheduler_with_aux.step(ade)
    current_lr = optimizer_with_aux.param_groups[0]['lr']
    print(f"Epoch {epoch+1}/150 - Train Loss: {train_loss:.4f}, ADE: {ade:.4f}, "
      f"FDE: {fde:.4f}, Traj MSE: {mse:.6f}, LR: {current_lr:.6f}")
    
    if ade < best_ade_with_aux:
        best_ade_with_aux = ade
        torch.save(model_with_aux.state_dict(), "vit_with_aux_model.pth")

print(f"\nTraining completed! Best validation ADE: {best_ade_with_aux:.4f}")
torch.save(model_with_aux.state_dict(), "best_vit_model.pth")
print("Saved best model as: best_vit_model.pth")

'''
print("\n" + "="*60)
print("COMPARISON RESULTS")
print("="*60)

model_no_aux.load_state_dict(torch.load("vit_no_aux_model.pth"))
model_with_aux.load_state_dict(torch.load("vit_with_aux_model.pth"))

ade_no_aux, fde_no_aux, mse_no_aux = validate_enhanced(model_no_aux, val_loader, device)
ade_with_aux, fde_with_aux, mse_with_aux = validate_enhanced(model_with_aux, val_loader, device)

print(f"Validation results for ViT model without auxiliary tasks:")
print(f"  ADE: {ade_no_aux:.4f}, FDE: {fde_no_aux:.4f}, Traj MSE: {mse_no_aux:.6f}")
print()
print(f"Validation results for ViT model with depth + semantic auxiliary tasks:")
print(f"  ADE: {ade_with_aux:.4f}, FDE: {fde_with_aux:.4f}, Traj MSE: {mse_with_aux:.6f}")
print()

improvement_ade = ((ade_no_aux - ade_with_aux) / ade_no_aux) * 100
improvement_fde = ((fde_no_aux - fde_with_aux) / fde_no_aux) * 100

print(f"IMPROVEMENT WITH AUXILIARY TASKS:")
print(f"  ADE improvement: {improvement_ade:.2f}%")
print(f"  FDE improvement: {improvement_fde:.2f}%")

if ade_with_aux < ade_no_aux:
    print(f"‚úÖ Auxiliary tasks helped! ADE improved by {improvement_ade:.2f}%")
else:
    print(f"‚ùå Auxiliary tasks didn't help. Consider tuning lambda values.")

if ade_with_aux < ade_no_aux:
    print(f"\nUsing model with auxiliary tasks for final submission (ADE: {ade_with_aux:.4f})")
    final_model = model_with_aux
    torch.save(model_with_aux.state_dict(), "best_vit_model.pth")
else:
    print(f"\nUsing model without auxiliary tasks for final submission (ADE: {ade_no_aux:.4f})")
    final_model = model_no_aux
    torch.save(model_no_aux.state_dict(), "best_vit_model.pth")

"""## üîç Final Visualization and Comparison

Now that we've trained two models ‚Äî one **with** the depth auxiliary task and one **without** ‚Äî let's visualize and compare their predictions.

We'll show:
1. The **camera image** from selected validation examples
2. The **past trajectory**, **ground-truth future**, and **predicted future** trajectory
3. The **predicted vs. ground-truth depth maps** (only for the model trained with the auxiliary task)

These visualizations help us understand:
- Does the predicted trajectory better match the future when the depth task is included?
- Is the predicted depth map reasonably accurate?

Let's see the difference! üìà
"""

import matplotlib.pyplot as plt
import random

random.seed(40)

def visualize_comparison(val_loader, model_no_aux, model_with_aux, device):
    model_no_aux.eval()
    model_with_aux.eval()
    val_batch = next(iter(val_loader))

    camera = val_batch['camera'].to(device)
    history = val_batch['history'].to(device)
    future = val_batch['future'].to(device)
    depth = val_batch['depth'].to(device)

    with torch.no_grad():
        pred_no_aux, _, _ = model_no_aux(camera, history)
        pred_with_aux, pred_depth, pred_semantic = model_with_aux(camera, history)

    camera = camera.cpu().numpy()
    history = history.cpu().numpy()
    future = future.cpu().numpy()
    pred_no_aux = pred_no_aux.cpu().numpy()
    pred_with_aux = pred_with_aux.cpu().numpy()
    depth = depth.cpu().numpy()
    pred_depth = pred_depth.cpu().numpy() if pred_depth is not None else None

    k = 4
    indices = random.choices(np.arange(len(camera)), k=k)

    fig, ax = plt.subplots(1, k, figsize=(4 * k, 4))
    for i, idx in enumerate(indices):
        ax[i].imshow(camera[idx].transpose(1, 2, 0))
        ax[i].set_title(f"Example {i+1}")
        ax[i].axis("off")
    plt.suptitle("Camera Inputs")
    plt.tight_layout()
    plt.show()

    fig, ax = plt.subplots(2, k, figsize=(4 * k, 8))
    for i, idx in enumerate(indices):
        ax[0, i].plot(history[idx, :, 0], history[idx, :, 1], 'o-', label='Past', color='gold', markersize=4, linewidth=1.2)
        ax[0, i].plot(future[idx, :, 0], future[idx, :, 1], 'o-', label='GT Future', color='green', markersize=4, linewidth=1.2)
        ax[0, i].plot(pred_no_aux[idx, :, 0], pred_no_aux[idx, :, 1], 'o-', label='Pred (No Aux)', color='red', markersize=4, linewidth=1.2)
        ax[0, i].set_title("No Depth Aux")
        ax[0, i].axis("equal")

        ax[1, i].plot(history[idx, :, 0], history[idx, :, 1], 'o-', label='Past', color='gold', markersize=4, linewidth=1.2)
        ax[1, i].plot(future[idx, :, 0], future[idx, :, 1], 'o-', label='GT Future', color='green', markersize=4, linewidth=1.2)
        ax[1, i].plot(pred_with_aux[idx, :, 0], pred_with_aux[idx, :, 1], 'o-', label='Pred (With Aux)', color='blue', markersize=4, linewidth=1.2)
        ax[1, i].set_title("With Depth Aux")
        ax[1, i].axis("equal")

    fig_legend = plt.figure(figsize=(8, 1))
    legend_handles = [
        plt.Line2D([0], [0], color='gold', marker='o', linestyle='-', markersize=5, label='Past'),
        plt.Line2D([0], [0], color='green', marker='o', linestyle='-', markersize=5, label='GT Future'),
        plt.Line2D([0], [0], color='red', marker='o', linestyle='-', markersize=5, label='Pred (No Aux)'),
        plt.Line2D([0], [0], color='blue', marker='o', linestyle='-', markersize=5, label='Pred (With Aux)')
    ]
    fig_legend.legend(handles=legend_handles, loc='center', ncol=4)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

    plt.suptitle("Trajectory Prediction: Without vs With Depth Aux Task")
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

    if pred_depth is not None:
        fig, ax = plt.subplots(2, k, figsize=(4 * k, 6))
        for i, idx in enumerate(indices):
            ax[0, i].imshow(depth[idx, :, :, 0], cmap='viridis')
            ax[0, i].set_title("GT Depth", pad=10)
            ax[0, i].axis("off")

            ax[1, i].imshow(pred_depth[idx, :, :, 0], cmap='viridis')
            ax[1, i].set_title("Pred Depth", pad=10)
            ax[1, i].axis("off")

        plt.suptitle("Depth Estimation (Only for Model With Aux Task)", y=1.05)
        plt.subplots_adjust(hspace=0.4)
        plt.tight_layout()
        plt.show()


visualize_comparison(val_loader, model_no_aux, model_with_aux, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
'''
"""Now we run our model on the test set once, to get the plan of our model and save it for submission. Notice that the ground truth plans are removed for the test set, so you can not calculate the ADE metric on the test set yourself, and need to submit it to the leader board. By running the last cell, you'll be able to see a csv file called submission_phase2.csv by clicking on the folder icon on the left. Download it and submit it to the leaderboard to get your score."""

with open(f"test_public/0.pkl", "rb") as f:
    data = pickle.load(f)
print(data.keys())

print("\n" + "="*60)
print("GENERATING TEST PREDICTIONS")
print("="*60)

with open(f"test_public/0.pkl", "rb") as f:
    data = pickle.load(f)
print("Test data keys:", data.keys())
print("Note: No 'sdc_future_feature' in test data (as expected)")

import pandas as pd
test_data_dir = "test_public"
test_files = [os.path.join(test_data_dir, fn) for fn in sorted([f for f in os.listdir(test_data_dir) if f.endswith(".pkl")], key=lambda fn: int(os.path.splitext(fn)[0]))]

test_dataset = EnhancedDrivingDataset(test_files, test=True, augment=False)
test_loader = DataLoader(test_dataset, batch_size=64, num_workers=2)

print(f"Loading best ViT model from: best_vit_model.pth")
final_model = ViTDrivingPlanner(
    use_depth_aux=True, 
    use_semantic_aux=True,
    model_name='vit_base_patch16_224'
)
final_model.load_state_dict(torch.load("best_vit_model.pth"))
final_model = final_model.to(device)
final_model.eval()

all_plans = []
print(f"Processing {len(test_files)} test samples...")
with torch.no_grad():
    for i, batch in enumerate(test_loader):
        camera = batch['camera'].to(device)
        history = batch['history'].to(device)
        
        pred_future, _, _ = final_model(camera, history)
        all_plans.append(pred_future.cpu().numpy()[..., :2])
        
        if (i + 1) % 10 == 0 or (i + 1) == len(test_loader):
            processed = min((i + 1) * test_loader.batch_size, len(test_files))
            print(f"  Processed {processed}/{len(test_files)} samples...")

all_plans = np.concatenate(all_plans, axis=0)

pred_xy = all_plans[..., :2]

total_samples, T, D = pred_xy.shape
pred_xy_flat = pred_xy.reshape(total_samples, T * D)

ids = np.arange(total_samples)
df_xy = pd.DataFrame(pred_xy_flat)
df_xy.insert(0, "id", ids)

new_col_names = ["id"]
for t in range(1, T + 1):
    new_col_names.append(f"x_{t}")
    new_col_names.append(f"y_{t}")
df_xy.columns = new_col_names

df_xy.to_csv("submission_phase2_vit.csv", index=False)

print(f"\nTest inference completed!")
print(f"Shape of df_xy: {df_xy.shape}")
print(f"Predictions saved to: submission_phase2_vit.csv")
